services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - backend
    restart: always
    networks:
      - app_network

  backend:
    build:
      context: ./
    container_name: backend
    ports:
      - "8000:8000"
      - "8080:8080"
    volumes:
      - ./llm/models:/app/llm/models
    restart: always
    networks:
      - app_network
    environment:
      - models_dir=/app/llm/models
      - llamacpp_binary=/app/llm/bin/llama-server
      - llamacpp_port=8080

networks:
  app_network:
    driver: bridge