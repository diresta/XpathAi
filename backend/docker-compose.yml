services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - backend
    restart: always
    networks:
      - app_network

  backend:
    build:
      context: ./
    container_name: backend
    ports:
      - "8000:8000"
      - "8080:8080"
    volumes:
      - ./llm/models:/app/llm/models
    restart: always
    networks:
      - app_network
    environment:
      - models_dir=/app/llm/models
      - llamacpp_binary=/app/llm/bin/llama-server
      - llamacpp_port=8080
      - max_tokens=512
      - generation_timeout=90
      - request_timeout=300
      - large_input_threshold=20000
      - CUDA_VISIBLE_DEVICES=all
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    mem_limit: 8g
    memswap_limit: 8g
    shm_size: 2g
    ulimits:
      memlock:
        soft: -1
        hard: -1

networks:
  app_network:
    driver: bridge