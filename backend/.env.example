models_dir=/app/llm/models
llamacpp_binary=/app/llm/llama-server
llamacpp_port=8080
default_model=model.gguf
max_tokens=512
temperature=0.7
request_timeout=300